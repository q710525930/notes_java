---
typora-copy-images-to: pic
typora-root-url: pic
---

# ElasticSearch

Elasticsearch 是一个**实时**的**分布式存储、搜索、分析**的引擎，相比于传统关系型数据库，具有几个优点：

- 模糊查询速度快（mysql模糊查询会返回大量信息，而且**不能走索引**）
- 根据评分排序
- 关键字适应度比较大



## ES数据结构

数据存储层级结构：

- **Index**：Elasticsearch的Index相当于数据库的Table
- **Type**：这个在新的Elasticsearch版本已经废除（在以前的Elasticsearch版本，一个Index下支持多个Type--有点类似于消息队列一个topic下多个group的概念）
- **Document**：Document相当于数据库的一行记录
- **Field**：相当于数据库的Column的概念
- **Mapping**：相当于数据库的Schema的概念
- **DSL**：相当于数据库的SQL（给我们读取Elasticsearch数据的API）

输入一段文字，Elastic 会索引所有字段进行分词，放入**Term Dictionary**，每个 Index （即数据库）的名字必须是小写。这段文字叫做文档，每个index存储了这些文档的id。

<img src="/Elasticsearch的数据结构.png" alt="image-20200711002613037" style="zoom:50%;" />

在`Term Dictionary`中的词由于是非常非常多的，所以我们会为其进行**排序**，等要查找的时候就可以通过**二分**来查，不需要遍历整个`Term Dictionary`。由于`Term Dictionary`的词实在太多了，不可能把`Term Dictionary`所有的词都放在内存中，于是Elasticsearch还抽了一层叫做`Term Index`，这层只存储  部分  **词的前缀**，`Term Index`会存在内存中（检索会特别快）。

`Term Index`在内存中是以**FST**（Finite State Transducers）的形式保存的，其特点是**非常节省内存**。FST有两个优点：

- 1）**空间占用小**。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；
- 2）**查询速度快**。O(len(str))的查询时间复杂度。

为什么Elasticsearch/Lucene检索可以比mysql快？

> Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。**检索一个term需要若干次的random access的磁盘操作**。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在**内存**中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。

`PostingList`会使用Frame Of Reference（**FOR**）编码技术对里边的数据进行压缩，**节约磁盘空间**。

>  比如一个词对应的文档id列表为[73, 300, 302, 332,343, 372] ，id列表首先要从小到大排好序；第一步增量编码就是从第二个数开始每个数存储与前一个id的差值，即300-73=227，302-300=2。。。，一直到最后一个数；第二步就是将这些差值放到不同的区块，Lucene使用256个区块，下面示例为了方便展示使用了3个区块，即每3个数一组；第三步位压缩，计算每组3个数中最大的那个数需要占用bit位数，比如30、11、29中最大数30最小需要5个bit位存储，这样11、29也用5个bit位存储，这样才占用15个bit，不到2个字节，压缩效果很好，如下面原理图所示：
>
> <img src="/Frame Of Reference原理图.png" alt="image-20200711014123976" style="zoom:50%;" />

而在内存中缓存主要是为了加速响应，我们查的时候往往需要对这些文档ID做**交集和并集**的操作（比如在多条件查询时)，`PostingList`使用**Roaring Bitmaps**来对文档ID进行交并集操作

> 简单的说就是在bitmap的基础上处理了仅有1,1024这种跨度很大的情况下对中间位置全为0的空间的浪费。核心就是用高16位表示在第几块，低16位表示在这块内的位置，所以一个32位数据最多存放65535（16位）块，每块能放65535个数据。
>
> 但是注意，当每个块内数据量小于4096时用short数组存储（4096个short占空间8192字节），大于4096才用bitmap。65535位用bitmap所需空间恒定为8192字节。

联合索引求交并集方案：

1. Skip List（硬盘中实现方案，未命中缓存）

   > 上面我们说到在硬盘中使用**FOR**技术来存储，那么硬盘中使用此方法求交并集实现逻辑为：
   >
   > <img src="/image-20200711014907744.png" alt="image-20200711014907744" style="zoom:50%;" />
   >
   > 上述三个posting list，进行合并，第一列遍历到13，那么其他列13以前的都可以跳过，这样方便元素跳跃的即是Skip List，
   >
   > 从概念上来说，对于一个很长的posting list，比如：
   >
   > [1,3,13,101,105,108,255,256,257]
   >
   > 我们可以把这个list分成三个block：
   >
   > [1,3,13] [101,105,108] [255,256,257]
   >
   > 然后可以构建出skip list的第二层：
   >
   > [1,101,255]
   >
   > 1,101,255分别指向自己对应的block。这样就可以很快地跨block的移动指向位置了。

2. bitSet

   > 对两个bitset做AN操作。

## ES架构

一个Elasticsearch集群会有多个Elasticsearch节点，节点中有一个`Master Node`，它主要负责维护索引元数据、负责切换主分片和副本分片身份等工作（后面会讲到分片的概念），如果主节点挂了，会选举出一个新的主节点。

一个Index的数据我们可以分发到不同的Node上进行存储，这个操作就叫做**分片**。（防止index数据太大撑爆节点，多个**节点并行查询**然后汇聚）

为了防止某个节点挂了导致分片丢失，分片也有主备，主写备读。

### 写入流程

请求发送到es集群，可以发送到任意一个节点，然后节点利用hash算法**充当路由转发给主节点**处理。消息到达主节点后主节点内部操作：

1. 将数据写到内存缓存区
2. 然后将数据写到translog缓存区
3. 每隔**1s**数据从buffer中refresh到FileSystemCache中，生成segment文件，一旦生成segment文件，就能通过索引查询到了（**Elasticsearch写入的数据需要1s才能查询到**）
4. refresh完，memory buffer就清空了。
5. 每隔**5s**中，translog 从buffer flush到磁盘中。（**Elasticsearch某个节点如果挂了，可能会造成有5s的数据丢失**）
6. 定期/定量从FileSystemCache中,结合translog内容`flush index`到磁盘中。

写内存缓冲区（**定时**去生成segement，生成translog），能够**让数据能被索引、被持久化**。最后通过commit完成一次的持久化。等主分片写完了以后，会将数据并行发送到副本集节点上，等到所有的节点写入成功就返回**ack**给协调节点，协调节点返回**ack**给客户端，完成一次的写入。

### 更新流程

Elasticsearch的更新和删除操作流程：

- 给对应的`doc`记录打上`.del`标识，如果是删除操作就打上`delete`状态，如果是更新操作就把原来的`doc`标志为`delete`，然后重新新写入一条数据

segments是不变的，所以文档不能从旧的segments中删除，也不能在旧的segments中更新来映射一个新的文档版本。前面提到了，每隔**1s**会生成一个segement 文件，那segement文件会越来越多越来越多。Elasticsearch会有一个**merge**任务，会将多个segement文件**合并**成一个segement文件。在合并的过程中，会把带有`delete`状态的`doc`给**物理删除**掉。

### 查询流程

查询有两种情况：

1. 根据id查doc
   - 检索内存的Translog文件
   - 检索硬盘的Translog文件
   - 检索硬盘的Segement文件
2. 根据query（搜索词）去查询匹配的doc
   - 同时查询内存和硬盘的segement

由于我们知道内存segement内秒生成一次，所以第二种是非实时的。

具体的ES内部查询流程（**QUERY_THEN_FETCH**）举例：

- 客户端请求发送到集群的某个节点上。集群上的每个节点都是coordinate node（协调节点）
- 然后协调节点将搜索的请求转发到**所有分片上**（主分片和副本分片都行）
- 每个分片将自己搜索出的结果`(doc id)`返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。
- 接着由协调节点根据 `doc id` 去各个节点上**拉取实际**的 `document` 数据，最终返回给客户端。

总体可以分为两个步骤：

**QUERY**：从各个节点都拉取对应的数据；协调节点向目标分片发送查询的命令，目标分片将结果**返回到本地有序的优先队列**中（在每个分片内做过滤、排序等等操作），返回`doc id`给协调节点**产生一个全局的排序列表**

**FETCH**：取数据，路由节点获取所有文档做**聚合**，返回给客户端。

## ES运行原理

### 什么是倒排索引

正常查询路径 书名->章节->内容，倒排索引正好反过来  内容->章节->书名。比如搜索引擎的通过关键字查找网页。

实现原理：底层基于**FST**（Finite State Transducer）数据结构实现。

### Master选主原理

Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分；

对所有可以成为master的节点（**node.master: true**）根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。

如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。

*补充：master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能*。

### 打分机制

总结一下就是：

1. 布尔模型：删选doc
2. TF-IDF词袋：进行打分，包括：
   - **词频**（术语在文档中出现的频度是多少？频度越高，权重越大）
   - **逆向文档频率**（集合所有文档里出现的频次。频次越高，权重越低）
   - **字段长度正则值**（字段越短，字段的权重越高）
   - **查询正则因子**
   - **查询协调**

查询完毕后**查询时权重提升**可以**使用权重提升参数让一个查询语句比其他语句更重要**。有些时候我们不关心 TF/IDF，我们只想知道一个词是否在某个字段中出现过，不关心它在文档中出现是否频繁。

[ElasticSearch相关性打分机制](https://zhuanlan.zhihu.com/p/27951938)

### 查询方式

[你必须知道的23个最有用的Elasticseaerch检索技巧](https://blog.csdn.net/laoyang360/article/details/76769208)

## 优化

### 查询性能优化杀手锏  **filesystem cache**

上述可知查询最终是查询的磁盘里面的数据，然后顺带放入`filesystem cache`缓存中，增加`filesystem cache`缓存的大小能够容纳更多segement即可有效提升查询速度。至少`filesystem cache`容量要占到总数据容量的一般，假设三台机器，每台64G内存，除开es 的jvm heap内存32G，就只剩32G，3台总共可用100G。

当`filesystem cache`内存容量不足时，就将文档数据**尽量只保存需要索引的少数字段**，完整的字段数据可以存入mysql或者hbase。这样就是从es查doc id，然后通过doc id去hbase里面查详细数据。

此外还可以做**数据预热**，隔一段时间将热点数据如微博大V刷到`filesystem cache`里面去，可以将热数据与冷数据分开不同索引存储，这样热数据的segement不容易被刷掉。

<img src="/filesystem cache.png" alt="image-20200711182834418" style="zoom:50%;" />

### 分页性能优化

es中查询第100页的10条数据，需要在每个节点里面查询到各自节点的第100页的10条数据，返回给协调节点再排序。所以总共要返回节点数*1000条数据，很蠢，也没办法，所以**es的指定分页查询性能很差**。可以采取微博下拉那种方式，直接返回所有信息，然后做个游标让用户只能按顺序往下挨个看，这样就不会让用户有分页需求了，解决需求的办法就是不让用户有这个需求。







高可用架构，水平扩容